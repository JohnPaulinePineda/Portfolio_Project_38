# [Supervised Learning : Exploring Boosting, Bagging and Stacking Algorithms for Ensemble Learning](https://johnpaulinepineda.github.io/Portfolio_Project_38/)

[<img src="https://img.shields.io/badge/R-blue?logoColor=blue&labelColor=white&style=for-the-badge" alt="R Badge"/>](https://www.r-project.org/) [<img src="https://img.shields.io/badge/RStudio-blue?logoColor=blue&labelColor=white&style=for-the-badge" alt="RStudio Badge"/>](https://posit.co/downloads/)

This [project](https://johnpaulinepineda.github.io/Portfolio_Project_38/) explores different ensemble learning approaches which combine the predictions from multiple models in an effort to achieve better predictive performance. The ensemble frameworks applied in the analysis were grouped into three classes including boosting models which add ensemble members sequentially that correct the predictions made by prior models and outputs a weighted average of the predictions; bagging models which fit many decision trees on different samples of the same dataset and averaging the predictions; and stacking which consolidate many different models types on the same data and using another model to learn how to best combine the predictions. Boosting models included the Adaptive Boosting, Stochastic Gradient Boosting and Extreme Gradient Boosting algorithms. Bagging models applied were the Random Forest and Bagged Classification and Regression Trees algorithms. Individual base learners including the Linear Discriminant Analysis, Classification and Regression Trees, Support Vector Machine (Radial Basis Function Kernel), K-Nearest Neighbors and Naive Bayes algorithms were evaluated for correlation and stacked together as contributors to the Logistic Regression and Random Forest meta-models. The resulting predictions derived from all ensemble learning models were evaluated based on their discrimination power using the area under the receiver operating characteristics curve (AUROC) metric.

<img src="images/Project38_Summary.png?raw=true"/>

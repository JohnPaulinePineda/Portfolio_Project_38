---
title: 'Supervised Learning : Exploring Boosting, Bagging and Stacking Algorithms for Ensemble Learning'
author: "John Pauline Pineda"
date: "August 19, 2023"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```

# **1. Table of Contents**
|
| Details.
|
| Details.
|
##  1.1 Sample Data
|
| The [<mark style="background-color: #EEEEEE;color: #FF0000">**Wisconsin Breast Cancer**</mark>](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data) dataset shared from the [<mark style="background-color: #CCECFF">**Kaggle**</mark>](https://www.kaggle.com/) website as obtained from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/) was used for this illustrated example.   
|
| Preliminary dataset assessment:
|
| **[A]** 1138 rows (observations)
| 
| **[B]** 32 columns (variables)
|      **[B.1]** 1/32 metadata (unique identifiers) = <span style="color: #FF0000">id</span> (numeric)
|      **[B.2]** 1/32 response = <span style="color: #FF0000">diagnosis</span> (factor)
|      **[B.3]** 30/32 predictors = 30/30 numeric
|             **[B.3.1]** <span style="color: #FF0000">radius_mean</span> (numeric)
|             **[B.3.2]** <span style="color: #FF0000">texture_mean</span> (numeric)
|             **[B.3.3]** <span style="color: #FF0000">perimeter_mean</span> (numeric)
|             **[B.3.4]** <span style="color: #FF0000">area_mean</span> (numeric)
|             **[B.3.5]** <span style="color: #FF0000">smoothness_mean</span> (numeric)
|             **[B.3.6]** <span style="color: #FF0000">compactness_mean</span> (numeric)
|             **[B.3.7]** <span style="color: #FF0000">concavity_mean</span> (numeric)
|             **[B.3.8]** <span style="color: #FF0000">concave.points_mean</span> (numeric)
|             **[B.3.9]** <span style="color: #FF0000">symmetry_mean</span> (numeric)
|             **[B.3.10]** <span style="color: #FF0000">fractal_dimension_mean</span> (numeric)
|             **[B.3.11]** <span style="color: #FF0000">radius_se</span> (numeric)
|             **[B.3.12]** <span style="color: #FF0000">texture_se</span> (numeric)
|             **[B.3.13]** <span style="color: #FF0000">perimeter_se</span> (numeric)
|             **[B.3.14]** <span style="color: #FF0000">area_se</span> (numeric)
|             **[B.3.15]** <span style="color: #FF0000">smoothness_se</span> (numeric)
|             **[B.3.16]** <span style="color: #FF0000">compactness_se</span> (numeric)
|             **[B.3.17]** <span style="color: #FF0000">concavity_se</span> (numeric)
|             **[B.3.18]** <span style="color: #FF0000">concave.points_se</span> (numeric)
|             **[B.3.19]** <span style="color: #FF0000">symmetry_se</span> (numeric)
|             **[B.3.20]** <span style="color: #FF0000">fractal_dimension_se</span> (numeric)
|             **[B.3.21]** <span style="color: #FF0000">radius_worst</span> (numeric)
|             **[B.3.22]** <span style="color: #FF0000">texture_worst</span> (numeric)
|             **[B.3.23]** <span style="color: #FF0000">perimeter_worst</span> (numeric)
|             **[B.3.24]** <span style="color: #FF0000">area_worst</span> (numeric)
|             **[B.3.25]** <span style="color: #FF0000">smoothness_worst</span> (numeric)
|             **[B.3.26]** <span style="color: #FF0000">compactness_worst</span> (numeric)
|             **[B.3.27]** <span style="color: #FF0000">concavity_worst</span> (numeric)
|             **[B.3.28]** <span style="color: #FF0000">concave.points_worst</span> (numeric)
|             **[B.3.29]** <span style="color: #FF0000">symmetry_worst</span> (numeric)
|             **[B.3.30]** <span style="color: #FF0000">fractal_dimension_worst</span> (numeric)
|
| 

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(AppliedPredictiveModeling)
library(tidyr)
library(caret)
library(lattice)
library(dplyr)
library(moments)
library(skimr)
library(RANN)
library(pls)
library(corrplot)
library(lares)
library(DMwR)
library(gridExtra)
library(rattle)
library(RColorBrewer)
library(stats)
library(caretEnsemble)

##################################
# Loading source and
# formulating the analysis set
##################################
BreastCancer <- read.csv("WisconsinBreastCancer.csv",
                   na.strings=c("NA","NaN"," ",""),
                   stringsAsFactors = FALSE)
BreastCancer <- as.data.frame(BreastCancer)

##################################
# Performing a general exploration of the data set
##################################
dim(BreastCancer)
str(BreastCancer)
summary(BreastCancer)

##################################
# Setting the data type
# for the response variable
##################################
BreastCancer$diagnosis <- factor(BreastCancer$diagnosis,
                                 levels = c("B","M"))

##################################
# Formulating a data type assessment summary
##################################
PDA <- BreastCancer
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)

```

</details>

##  1.2 Data Quality Assessment
|
| **[A]** No missing observations noted for any predictor.
|
| **[B]** Low variance observed for 1 predictor with First.Second.Mode.Ratio>5.
|      **[B.1]** <span style="color: #FF0000">concavity_se</span> = 6.50
|
| **[C]** No low variance observed for any predictor with Unique.Count.Ratio<0.01.
|
| **[D]** High skewness observed for 5 predictors with Skewness>3 or Skewness<(-3).
|      **[D.1]** <span style="color: #FF0000">radius_se</span> = +3.08
|      **[D.2]** <span style="color: #FF0000">perimeter_se</span> = +3.43
|      **[D.3]** <span style="color: #FF0000">area_se</span> = +5.43
|      **[D.4]** <span style="color: #FF0000">concavity_se</span> = +5.10
|      **[D.5]** <span style="color: #FF0000">fractal_dimension_se</span> = +3.91
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- BreastCancer

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Name= names(DQA),
  Column.Type=sapply(DQA, function(x) class(x)),
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all Predictors
##################################
DQA.Predictors <- DQA[,!names(DQA) %in% c("id","diagnosis")]

##################################
# Listing all numeric Predictors
##################################
DQA.Predictors.Numeric <- DQA.Predictors[,sapply(DQA.Predictors, is.numeric)]

if (length(names(DQA.Predictors.Numeric))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Numeric))),
               " numeric predictor variable(s)."))
} else {
  print("There are no numeric predictor variables.")
}

##################################
# Listing all factor Predictors
##################################
DQA.Predictors.Factor <- DQA.Predictors[,sapply(DQA.Predictors, is.factor)]

if (length(names(DQA.Predictors.Factor))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Factor))),
               " factor predictor variable(s)."))
} else {
  print("There are no factor predictor variables.")
}

##################################
# Formulating a data quality assessment summary for factor Predictors
##################################
if (length(names(DQA.Predictors.Factor))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Predictors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Factor),
  Column.Type=sapply(DQA.Predictors.Factor, function(x) class(x)),
  Unique.Count=sapply(DQA.Predictors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Formulating a data quality assessment summary for numeric Predictors
##################################
if (length(names(DQA.Predictors.Numeric))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Predictors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Numeric),
  Column.Type=sapply(DQA.Predictors.Numeric, function(x) class(x)),
  Unique.Count=sapply(DQA.Predictors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Predictors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Predictors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Predictors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Predictors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Predictors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Predictors.Numeric, function(x) format(round(kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance Predictors
##################################
if (length(names(DQA.Predictors.Factor))==0) {
  print("No factor predictors noted.")
} else if (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric predictors due to low unique count ratio noted.")
}

##################################
# Checking for skewed Predictors
##################################
if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric predictors noted.")
}

```

</details>

##  1.3 Data Preprocessing

###  1.3.1 Outlier Detection
|
| **[A]** Outliers noted for 29 out of the 30 predictors. Predictor values were visualized through a boxplot including observations classified as suspected outliers using the IQR criterion. The IQR criterion means that all observations above the (75th percentile + 1.5 x IQR) or below the (25th percentile - 1.5 x IQR) are suspected outliers, where IQR is the difference between the third quartile (75th percentile) and first quartile (25th percentile).
|      **[A.1]** <span style="color: #FF0000">radius_mean</span> = 28
|      **[A.2]** <span style="color: #FF0000">texture_mean</span> = 14
|      **[A.3]** <span style="color: #FF0000">perimeter_mean</span> = 26
|      **[A.4]** <span style="color: #FF0000">area_mean</span> = 50
|      **[A.5]** <span style="color: #FF0000">smoothness_mean</span> = 12
|      **[A.6]** <span style="color: #FF0000">compactness_mean</span> = 32
|      **[A.7]** <span style="color: #FF0000">concavity_mean</span> = 36
|      **[A.8]** <span style="color: #FF0000">concave.points_mean</span> = 20
|      **[A.9]** <span style="color: #FF0000">symmetry_mean</span> = 30
|      **[A.10]** <span style="color: #FF0000">fractal_dimension_mean</span> = 30
|      **[A.11]** <span style="color: #FF0000">radius_se</span> = 76
|      **[A.12]** <span style="color: #FF0000">texture_se</span> = 40
|      **[A.13]** <span style="color: #FF0000">perimeter_se</span> = 76
|      **[A.14]** <span style="color: #FF0000">area_se</span> = 130
|      **[A.15]** <span style="color: #FF0000">smoothness_se</span> = 60
|      **[A.16]** <span style="color: #FF0000">compactness_se</span> = 56
|      **[A.17]** <span style="color: #FF0000">concavity_se</span> = 44
|      **[A.18]** <span style="color: #FF0000">concave.points_se</span> = 38
|      **[A.19]** <span style="color: #FF0000">symmetry_se</span> = 54
|      **[A.20]** <span style="color: #FF0000">fractal_dimension_se</span> = 56
|      **[A.21]** <span style="color: #FF0000">radius_worst</span> = 34
|      **[A.22]** <span style="color: #FF0000">texture_worst</span> = 10
|      **[A.23]** <span style="color: #FF0000">perimeter_worst</span> = 30
|      **[A.24]** <span style="color: #FF0000">area_worst</span> = 70
|      **[A.25]** <span style="color: #FF0000">smoothness_worst</span> = 14
|      **[A.26]** <span style="color: #FF0000">compactness_worst</span> = 32
|      **[A.27]** <span style="color: #FF0000">concavity_worst</span> = 24
|      **[A.28]** <span style="color: #FF0000">symmetry_worst</span> = 46
|      **[A.29]** <span style="color: #FF0000">fractal_dimension_worst</span> = 48
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.1.1, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- DQA[,!names(DQA) %in% c("id")]

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA)) 

```

```{r section_1.3.1.2, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}

##################################
# Outlier Detection
##################################

##################################
# Listing all Predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("diagnosis")]

##################################
# Listing all numeric Predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Identifying outliers for the numeric Predictors
##################################
OutlierCountList <- c()

for (i in 1:ncol(DPA.Predictors.Numeric)) {
  Outliers <- boxplot.stats(DPA.Predictors.Numeric[,i])$out
  OutlierCount <- length(Outliers)
  OutlierCountList <- append(OutlierCountList,OutlierCount)
  OutlierIndices <- which(DPA.Predictors.Numeric[,i] %in% c(Outliers))
  print(
  ggplot(DPA.Predictors.Numeric, aes(x=DPA.Predictors.Numeric[,i])) +
  geom_boxplot() +
  theme_bw() +
  theme(axis.text.y=element_blank(), 
        axis.ticks.y=element_blank()) +
  xlab(names(DPA.Predictors.Numeric)[i]) +
  labs(title=names(DPA.Predictors.Numeric)[i],
       subtitle=paste0(OutlierCount, " Outlier(s) Detected")))
}

```

</details>

###  1.3.2 Zero and Near-Zero Variance
|
| **[A]** No low variance observed for any predictor using a preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package. The <span style="color: #0000FF">nearZeroVar</span> method using both the <span style="color: #0000FF">freqCut</span> and <span style="color: #0000FF">uniqueCut</span> criteria set at 95/5 and 10, respectively, were applied on the dataset.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.2, warning=FALSE, message=FALSE}

##################################
# Zero and Near-Zero Variance
##################################

##################################
# Identifying columns with low variance
###################################
DPA_LowVariance <- nearZeroVar(DPA,
                               freqCut = 80/20,
                               uniqueCut = 10,
                               saveMetrics= TRUE)
(DPA_LowVariance[DPA_LowVariance$nzv,])

if ((nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))==0){
  
  print("No low variance descriptors noted.")
  
} else {

  print(paste0("Low variance observed for ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s) with First.Second.Mode.Ratio>4 and Unique.Count.Ratio<0.10."))
  
  DPA_LowVarianceForRemoval <- (nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))
  
  print(paste0("Low variance can be resolved by removing ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LowVarianceForRemoval) {
  DPA_LowVarianceRemovedVariable <- rownames(DPA_LowVariance[DPA_LowVariance$nzv,])[j]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LowVarianceRemovedVariable))
  }
  
  DPA %>%
  skim() %>%
  dplyr::filter(skim_variable %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,]))

}

```

</details>

###  1.3.3 Collinearity
|
| **[A]** High correlation values were noted for 15 pairs of numeric predictors with Pearson correlation coefficients >80% as confirmed using the preprocessing summaries from the <mark style="background-color: #CCECFF">**caret**</mark> package.
|      **[A.1]** <span style="color: #FF0000">radius_mean</span> and <span style="color: #FF0000">perimeter_mean</span>  = +100%
|      **[A.2]** <span style="color: #FF0000">radius_worst</span> and <span style="color: #FF0000">perimeter_worst</span>  = +99%
|      **[A.3]** <span style="color: #FF0000">radius_mean</span> and <span style="color: #FF0000">area_mean</span>  = +99%
|      **[A.4]** <span style="color: #FF0000">perimeter_mean</span> and <span style="color: #FF0000">area_mean</span>  = +99%
|      **[A.5]** <span style="color: #FF0000">radius_worst</span> and <span style="color: #FF0000">area_worst</span>  = +98%
|      **[A.6]** <span style="color: #FF0000">perimeter_worst</span> and <span style="color: #FF0000">area_worst</span>  = +98%
|      **[A.7]** <span style="color: #FF0000">radius_se</span> and <span style="color: #FF0000">perimeter_se</span>  = +97%
|      **[A.8]** <span style="color: #FF0000">perimeter_mean</span> and <span style="color: #FF0000">perimeter_worst</span>  = +97%
|      **[A.9]** <span style="color: #FF0000">radius_mean</span> and <span style="color: #FF0000">radius_worst</span>  = +97%
|      **[A.10]** <span style="color: #FF0000">perimeter_mean</span> and <span style="color: #FF0000">radius_worst</span>  = +97%%
|      **[A.11]** <span style="color: #FF0000">radius_mean</span> and <span style="color: #FF0000">perimeter_worst</span>  = +96%
|      **[A.12]** <span style="color: #FF0000">area_mean</span> and <span style="color: #FF0000">radius_worst</span>  = +96%
|      **[A.13]** <span style="color: #FF0000">area_mean</span> and <span style="color: #FF0000">area_worst</span>  = +96%
|      **[A.14]** <span style="color: #FF0000">area_mean</span> and <span style="color: #FF0000">perimeter_worst</span>  = +96%
|      **[A.15]** <span style="color: #FF0000">radius_se</span> and <span style="color: #FF0000">area_se</span>  = +95%
|
| **[B]** 7 predictors driving high pairwise correlation were recommended for removal using the <span style="color: #0000FF">findCorrelation</span> preprocessing method from the <mark style="background-color: #CCECFF">**caret**</mark> package. The function looks at the mean absolute correlation of each predictor and removes that with the largest mean absolute correlation.
|      **[B.1]** <span style="color: #FF0000">perimeter_worst</span>
|      **[B.2]** <span style="color: #FF0000">radius_worst</span>
|      **[B.3]** <span style="color: #FF0000">perimeter_mean</span>
|      **[B.4]** <span style="color: #FF0000">area_worst</span>
|      **[B.5]** <span style="color: #FF0000">radius_mean</span>
|      **[B.6]** <span style="color: #FF0000">perimeter_se</span>
|      **[B.7]** <span style="color: #FF0000">area_se</span>
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3, warning=FALSE, message=FALSE}

##################################
# Visualizing pairwise correlation between Predictor
##################################
(DPA_Correlation <- cor(DPA.Predictors.Numeric,
                        method = "pearson",
                        use="pairwise.complete.obs"))

DPA_CorrelationTest <- cor.mtest(DPA.Predictors.Numeric,
                       method = "pearson",
                       conf.level = 0.95)

corrplot(cor(DPA.Predictors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"),
             method = "circle",
             type = "upper",
             order = "original",
             tl.col = "black",
             tl.cex = 0.75,
             tl.srt = 90,
             sig.level = 0.05,
             p.mat = DPA_CorrelationTest$p,
             insig = "blank")

corrplot(cor(DPA.Predictors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"),
             method = "number",
             type = "upper",
             order = "original",
             tl.col = "black",
             tl.cex = 0.75,
             tl.srt = 90,
             sig.level = 0.05,
             number.cex = 0.65,
             p.mat = DPA_CorrelationTest$p,
             insig = "blank")

##################################
# Identifying the highly correlated variables
##################################
(DPA_HighlyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)])>0.95))

if (DPA_HighlyCorrelatedCount == 0) {
  print("No highly correlated predictors noted.")
} else {
  print(paste0("High correlation observed for ",
               (DPA_HighlyCorrelatedCount),
               " pairs of numeric variable(s) with Correlation.Coefficient>0.95."))
  
  (DPA_HighlyCorrelatedPairs <- corr_cross(DPA.Predictors.Numeric,
  max_pvalue = 0.05, 
  top = DPA_HighlyCorrelatedCount,
  rm.na = TRUE,
  grid = FALSE
))
  
}

if (DPA_HighlyCorrelatedCount > 0) {
  DPA_HighlyCorrelated <- findCorrelation(DPA_Correlation, cutoff = 0.95)

  (DPA_HighlyCorrelatedForRemoval <- length(DPA_HighlyCorrelated))

  print(paste0("High correlation can be resolved by removing ",
               (DPA_HighlyCorrelatedForRemoval),
               " numeric variable(s)."))

  for (j in 1:DPA_HighlyCorrelatedForRemoval) {
  DPA_HighlyCorrelatedRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_HighlyCorrelated[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_HighlyCorrelatedRemovedVariable))
  }

}

```

</details>

###  1.3.4 Linear Dependency
|
| **[A]** No linear dependencies noted for any subset of numeric variables using the preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package applying the <span style="color: #0000FF">findLinearCombos</span> method which utilizes the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). 
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4, warning=FALSE, message=FALSE}

##################################
# Linear Dependencies
##################################

##################################
# Finding linear dependencies
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)

##################################
# Identifying the linearly dependent variables
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)

(DPA_LinearlyDependentCount <- length(DPA_LinearlyDependent$linearCombos))

if (DPA_LinearlyDependentCount == 0) {
  print("No linearly dependent predictors noted.")
} else {
  print(paste0("Linear dependency observed for ",
               (DPA_LinearlyDependentCount),
               " subset(s) of numeric variable(s)."))
  
  for (i in 1:DPA_LinearlyDependentCount) {
    DPA_LinearlyDependentSubset <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$linearCombos[[i]]]
    print(paste0("Linear dependent variable(s) for subset ",
                 i,
                 " include: ",
                 DPA_LinearlyDependentSubset))
  }
  
}

##################################
# Identifying the linearly dependent variables for removal
##################################

if (DPA_LinearlyDependentCount > 0) {
  DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)
  
  DPA_LinearlyDependentForRemoval <- length(DPA_LinearlyDependent$remove)
  
  print(paste0("Linear dependency can be resolved by removing ",
               (DPA_LinearlyDependentForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LinearlyDependentForRemoval) {
  DPA_LinearlyDependentRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$remove[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LinearlyDependentRemovedVariable))
  }

}


```

</details>

###  1.3.5 Distributional Shape
|
| **[A]** Shape transformation was applied to improve against skewness and minimize outliers for data distribution stability using the <span style="color: #0000FF">BoxCox</span> method from the <mark style="background-color: #CCECFF">**caret**</mark> package which transforms the distributional shape for predictors with strictly positive values.
|
| **[B]** Skewness measurements were improved for most except for 1 predictor with Skewness>3. 
|      **[B.1]** <span style="color: #FF0000">concavity_se</span> = +5.10
|
| **[C]** Outliers were minimized for most except for 5 predictors which did not show any improvement even after shape transformation as noted using the IQR criterion.
|      **[C.1]** <span style="color: #FF0000">concavity_mean</span> = 36
|      **[C.2]** <span style="color: #FF0000">concave.points_mean</span> = 20
|      **[C.3]** <span style="color: #FF0000">concavity_se</span> = 44
|      **[C.4]** <span style="color: #FF0000">concave.points_se</span> = 38
|      **[C.5]** <span style="color: #FF0000">concavity_worst</span> = 24
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.5, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}

##################################
# Shape Transformation
##################################

##################################
# Applying a Box-Cox transformation
##################################
DPA_BoxCox <- preProcess(DPA.Predictors.Numeric, method = c("BoxCox"))
DPA_BoxCoxTransformed <- predict(DPA_BoxCox, DPA.Predictors.Numeric)

for (i in 1:ncol(DPA_BoxCoxTransformed)) {
  Median <- format(round(median(DPA_BoxCoxTransformed[,i],na.rm = TRUE),2), nsmall=2)
  Mean <- format(round(mean(DPA_BoxCoxTransformed[,i],na.rm = TRUE),2), nsmall=2)
  Skewness <- format(round(skewness(DPA_BoxCoxTransformed[,i],na.rm = TRUE),2), nsmall=2)
  print(
  ggplot(DPA_BoxCoxTransformed, aes(x=DPA_BoxCoxTransformed[,i])) +
  geom_histogram(binwidth=1,color="black", fill="white") +
  geom_vline(aes(xintercept=mean(DPA_BoxCoxTransformed[,i])),
            color="blue", size=1) +
    geom_vline(aes(xintercept=median(DPA_BoxCoxTransformed[,i])),
            color="red", size=1) +
  theme_bw() +
  ylab("Count") +
  xlab(names(DPA_BoxCoxTransformed)[i]) +
  labs(title=names(DPA_BoxCoxTransformed)[i],
       subtitle=paste0("Median = ", Median,
                       ", Mean = ", Mean,
                       ", Skewness = ", Skewness)))
}

##################################
# Identifying outliers for the numeric predictors
##################################
OutlierCountList <- c()

for (i in 1:ncol(DPA_BoxCoxTransformed)) {
  Outliers <- boxplot.stats(DPA_BoxCoxTransformed[,i])$out
  OutlierCount <- length(Outliers)
  OutlierCountList <- append(OutlierCountList,OutlierCount)
  OutlierIndices <- which(DPA_BoxCoxTransformed[,i] %in% c(Outliers))
  print(
  ggplot(DPA_BoxCoxTransformed, aes(x=DPA_BoxCoxTransformed[,i])) +
  geom_boxplot() +
  theme_bw() +
  theme(axis.text.y=element_blank(), 
        axis.ticks.y=element_blank()) +
  xlab(names(DPA_BoxCoxTransformed)[i]) +
  labs(title=names(DPA_BoxCoxTransformed)[i],
       subtitle=paste0(OutlierCount, " Outlier(s) Detected")))
}

DPA_BoxCoxTransformed$diagnosis <- DPA[,c("diagnosis")]

```

</details>

###  1.3.6 Pre-Processed Dataset
|
| **[A]**  A total of 12 predictors were removed prior to data exploration and modelling due to issues identified during data preprocessing.
|      **[A.1]** <span style="color: #FF0000">concavity_se</span> = Low variance and high skewness
|      **[A.2]** <span style="color: #FF0000">perimeter_worst</span> = High correlation with <span style="color: #FF0000">radius_worst</span>, <span style="color: #FF0000">area_worst</span>, <span style="color: #FF0000">perimeter_mean</span>, <span style="color: #FF0000">radius_mean</span> and <span style="color: #FF0000">area_mean</span>
|      **[A.3]** <span style="color: #FF0000">radius_worst</span> = High correlation with <span style="color: #FF0000">perimeter_worst</span>, <span style="color: #FF0000">area_worst</span>, <span style="color: #FF0000">radius_mean</span>, <span style="color: #FF0000">perimeter_mean</span> and <span style="color: #FF0000">area_mean</span>
|      **[A.4]** <span style="color: #FF0000">perimeter_mean</span> = High correlation with <span style="color: #FF0000">radius_mean</span>, <span style="color: #FF0000">area_mean</span>, <span style="color: #FF0000">perimeter_worst</span> and <span style="color: #FF0000">radius_worst</span>
|      **[A.5]** <span style="color: #FF0000">area_worst</span> = High correlation with <span style="color: #FF0000">radius_worst</span>, <span style="color: #FF0000">perimeter_worst</span> and <span style="color: #FF0000">area_mean</span>
|      **[A.6]** <span style="color: #FF0000">radius_mean</span> = High correlation with <span style="color: #FF0000">perimeter_mean</span>, <span style="color: #FF0000">area_mean</span>, <span style="color: #FF0000">radius_worst</span> and <span style="color: #FF0000">perimeter_worst </span>.
|      **[A.7]** <span style="color: #FF0000">perimeter_se</span> = High correlation with <span style="color: #FF0000">radius_se</span>
|      **[A.8]** <span style="color: #FF0000">area_se</span> = High correlation with <span style="color: #FF0000">radius_se</span>
|      **[A.9]** <span style="color: #FF0000">concavity_mean</span> = High outlier count even after shape transformation
|      **[A.10]** <span style="color: #FF0000">concave.points_mean</span> = High outlier count even after shape transformation
|      **[A.11]** <span style="color: #FF0000">concave.points_se</span> = High outlier count even after shape transformation
|      **[A.12]** <span style="color: #FF0000">concavity_worst</span> = High outlier count even after shape transformation
|
| **[B]** The preprocessed tabular dataset was comprised of 1138 observations and 19 variables (including 1 response and 18 predictors). 
|      **[B.1]** 1138 rows (observations)
|      **[B.2]** 19 columns (variables)
|             **[B.2.1]** 1/19 response = <span style="color: #FF0000">diagnosis</span> (factor)
|             **[B.2.2]** 18/19 predictors = 18/18 numeric
|                      **[B.2.2.1]** <span style="color: #FF0000">texture_mean</span> (numeric)
|                      **[B.2.2.2]** <span style="color: #FF0000">area_mean</span> (numeric)
|                      **[B.2.2.3]** <span style="color: #FF0000">smoothness_mean</span> (numeric)
|                      **[B.2.2.4]** <span style="color: #FF0000">compactness_mean</span> (numeric)
|                      **[B.2.2.5]** <span style="color: #FF0000">symmetry_mean	</span> (numeric)
|                      **[B.2.2.6]** <span style="color: #FF0000">fractal_dimension_mean</span> (numeric)
|                      **[B.2.2.7]** <span style="color: #FF0000">radius_se</span> (numeric)
|                      **[B.2.2.8]** <span style="color: #FF0000">texture_se</span> (numeric)
|                      **[B.2.2.9]** <span style="color: #FF0000">smoothness_se</span> (numeric)
|                      **[B.2.2.10]** <span style="color: #FF0000">compactness_se</span> (numeric)
|                      **[B.2.2.11]** <span style="color: #FF0000">symmetry_se</span> (numeric)
|                      **[B.2.2.12]** <span style="color: #FF0000">fractal_dimension_se</span> (numeric)
|                      **[B.2.2.13]** <span style="color: #FF0000">texture_worst</span> (numeric)
|                      **[B.2.2.14]** <span style="color: #FF0000">smoothness_worst</span> (numeric)
|                      **[B.2.2.15]** <span style="color: #FF0000">compactness_worst</span> (numeric)
|                      **[B.2.2.16]** <span style="color: #FF0000">concave.points_worst</span> (numeric)
|                      **[B.2.2.17]** <span style="color: #FF0000">symmetry_worst</span> (numeric)
|                      **[B.2.2.18]** <span style="color: #FF0000">fractal_dimension_worst</span> (numeric)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}
##################################
# Creating the pre-modelling
# train set
##################################
PMA <- DPA_BoxCoxTransformed[,!names(DPA_BoxCoxTransformed) %in% c("concavity_se",
                                                                   "perimeter_worst",
                                                                   "radius_worst",
                                                                   "perimeter_mean",
                                                                   "area_worst",
                                                                   "radius_mean",
                                                                   "perimeter_se",
                                                                   "area_se",
                                                                   "concavity_mean",
                                                                   "concave.points_mean",
                                                                   "concave.points_se",
                                                                   "concavity_worst")]

##################################
# Gathering descriptive statistics
##################################
(PMA_Skimmed <- skim(PMA))

```

</details>

## 1.4 Data Exploration
|
| **[A]** Individual predictors which demonstrated excellent discrimination between <span style="color: #FF0000">diagnosis=M</span> and <span style="color: #FF0000">diagnosis=B</span> in terms of the area under the receiver operating characteristics curve (AUROC) are as follows:
|      **[A.1]** <span style="color: #FF0000">concave.points_worst</span> = 0.97
|      **[A.2]** <span style="color: #FF0000">area_mean</span> = 0.94
|      **[A.3]** <span style="color: #FF0000">radius_se</span> = 0.87
|      **[A.4]** <span style="color: #FF0000">compactness_mean</span> = 0.86
|      **[A.5]** <span style="color: #FF0000">compactness_worst</span> = 0.86
|      **[A.6]** <span style="color: #FF0000">texture_worst</span> = 0.78
|      **[A.7]** <span style="color: #FF0000">texture_mean</span> = 0.77
|      **[A.8]** <span style="color: #FF0000">smoothness_worst</span> = 0.75
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- PMA

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("diagnosis")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]
ncol(DPA.Predictors.Numeric)

##################################
# Converting response variable data type to factor
##################################
DPA$diagnosis <- as.factor(DPA$diagnosis)
length(levels(DPA$diagnosis))

##################################
# Formulating the box plots
##################################
featurePlot(x = DPA.Predictors.Numeric, 
            y = DPA$diagnosis,
            plot = "box",
            scales = list(x = list(relation="free", rot = 90), 
                          y = list(relation="free")),
            adjust = 1.5, 
            pch = "|", 
            layout = c(6, 3))

##################################
# Obtaining the AUROC
##################################
AUROC <- filterVarImp(x = DPA.Predictors.Numeric,
                        y = DPA$diagnosis)

##################################
# Formulating the summary table
##################################
AUROC_Summary <- AUROC 

AUROC_Summary$Predictor <- rownames(AUROC)
names(AUROC_Summary)[1] <- "AUROC"
AUROC_Summary$Metric <- rep("AUROC",nrow(AUROC))

AUROC_Summary[order(AUROC_Summary$AUROC, decreasing=TRUE),] 

##################################
# Exploring predictor performance
##################################
dotplot(Predictor ~ AUROC | Metric, 
        AUROC_Summary,
        origin = 0,
        type = c("p", "h"),
        pch = 16,
        cex = 2,
        alpha = 0.45,
        prepanel = function(x, y) {
            list(ylim = levels(reorder(y, x)))
        },
        panel = function(x, y, ...) {
            panel.dotplot(x, reorder(y, x), ...)
        })

##################################
# Creating the pre-modelling dataset
# into the train and test sets
##################################
set.seed(12345678)
MA_Train_Index  <- createDataPartition(DPA$diagnosis,p=0.8)[[1]]
MA_Train        <- DPA[ MA_Train_Index, ]
MA_Test         <- DPA[-MA_Train_Index, ]

```

</details>

## 1.5 Model Boosting

###  1.5.1 Adaptive Boosting (MBS_AB)
|
| Details.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.1, warning=FALSE, message=FALSE}

```

</details>

###  1.5.2 Stochastic Gradient Boosting (MBS_GBM)
|
| Details.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.2, warning=FALSE, message=FALSE}

```

</details>

###  1.5.3 Extreme Gradient Boosting (MBS_XGB)
|
| Details.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.3, warning=FALSE, message=FALSE}

```

</details>

## 1.6 Model Bagging

###  1.6.1 Random Forest (MBG_RF)
|
| Details.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6.1, warning=FALSE, message=FALSE}

```

</details>

###  1.6.2 Bagged Classification and Regression Trees (MBG_BTREE)
|
| Details.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6.2, warning=FALSE, message=FALSE}

```

</details>

## 1.7 Model Stacking

###  1.7.1 Base Learner Model Development using Linear Discriminant Analysis (BAL_LDA)
|
| Details.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.7.1, warning=FALSE, message=FALSE}

```

</details>

###  1.7.2 Base Learner Model Development using Classification and Regression Trees (BAL_CART)
|
| Details.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.7.2, warning=FALSE, message=FALSE}

```

</details>

###  1.7.3 Base Learner Model Development using Support Vector Machine - Radial Basis Function Kernel (BAL_SVM_R)
|
| Details.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.7.3, warning=FALSE, message=FALSE}

```

</details>

###  1.7.4 Base Learner Model Development using K-Nearest Neighbors (BAL_KNN)
|
| Details.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.7.4, warning=FALSE, message=FALSE}

```

</details>

###  1.7.5 Base Learner Model Development using Naive Bayes (BAL_NB)
|
| Details.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.7.5, warning=FALSE, message=FALSE}

```

</details>


###  1.7.6 Meta Learner Model Development using Linear Regression (MEL_LR)
|
| Details.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.7.6, warning=FALSE, message=FALSE}

```

</details>

###  1.7.7 Meta Learner Model Development using Random Forest (MEL_RF)
|
| Details.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.7.7, warning=FALSE, message=FALSE}

```

</details>

##  1.8 Algorithm Comparison Summary
|
| Details.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.8, warning=FALSE, message=FALSE, dev='png'}

```

</details>

|
# **2. References**
|
| **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson
| **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani
| **[Book]** [Multivariate Data Visualization with R](http://lmdvr.r-forge.r-project.org/figures/figures.html) by Deepayan Sarkar
| **[Book]** [Machine Learning](https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/) by Samuel Jackson
| **[Book]** [Data Modeling Methods](https://bookdown.org/larget_jacob/data-modeling-methods/) by Jacob Larget
| **[Book]** [Introduction to R and Statistics](https://saestatsteaching.tech/) by University of Western Australia
| **[Book]** [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by Max Kuhn and Kjell Johnson
| **[Book]** [Introduction to Research Methods](https://bookdown.org/ejvanholm/Textbook/) by Eric van Holm
| **[R Package]** [AppliedPredictiveModeling](https://cran.r-project.org/web//packages/AppliedPredictiveModeling/AppliedPredictiveModeling.pdf) by Max Kuhn
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [rpart](https://mran.microsoft.com/web/packages/rpart/rpart.pdf) by Terry Therneau and Beth Atkinson
| **[R Package]** [lattice](https://cran.r-project.org/web/packages/lattice/lattice.pdf) by  Deepayan Sarkar
| **[R Package]** [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html/) by Hadley Wickham
| **[R Package]** [tidyr](https://cran.r-project.org/web/packages/tidyr/tidyr.pdf) by Hadley Wickham
| **[R Package]** [moments](https://cran.r-project.org/web/packages/moments/index.html) by Lukasz Komsta and Frederick
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [RANN](https://cran.r-project.org/web/packages/RANN/RANN.pdf) by  Sunil Arya, David Mount, Samuel Kemp and Gregory Jefferis
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [tidyverse](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf) by Hadley Wickham
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [DMwR](https://mran.microsoft.com/snapshot/2016-05-02/web/packages/DMwR/DMwR.pdf) by Luis Torgo
| **[R Package]** [gridExtra](https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf) by Baptiste Auguie and Anton Antonov
| **[R Package]** [rattle](https://cran.r-project.org/web/packages/rattle/rattle.pdf) by Graham Williams
| **[R Package]** [RColorBrewer](https://cran.r-project.org/web//packages/RColorBrewer/RColorBrewer.pdf) by Erich Neuwirth
| **[R Package]** [stats](https://search.r-project.org/R/refmans/stats/html/00Index.html) by R Core Team

| **[R Package]** [caretEnsemble](https://cran.r-project.org/web/packages/caretEnsemble/caretEnsemble.pdf) by Zachary Deane-Mayer
| **[Article]** [A Brief Introduction to caretEnsemble)](https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html) by Zachary Deane-Mayer
| **[Article]** [How to Build an Ensemble Of Machine Learning Algorithms in R](https://machinelearningmastery.com/machine-learning-ensembles-with-r/) by Jason Brownlee
| **[Article]** [Ensemble Learning: Bagging, Boosting, and Stacking](https://towardsai.net/p/machine-learning/ensemble-learning-bagging-boosting-and-stacking) by Towards AI Team
| **[Article]** [Bagging, Boosting, and Stacking in Machine Learning](https://www.baeldung.com/cs/bagging-boosting-stacking-ml-ensemble-models) by Emmanuella Budu
| **[Article]** [Stacking Ensemble Machine Learning With Python](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/) by Jason Brownlee
| **[Article]** [Essence of Boosting Ensembles for Machine Learning](https://machinelearningmastery.com/essence-of-boosting-ensembles-for-machine-learning/) by Jason Brownlee
| **[Article]** [Ensemble Modeling with R](https://www.pluralsight.com/guides/ensemble-modeling-with-r) by Deepika Singh
| **[Article]** [Creating Ensemble Models in R](https://dustinrogers.github.io/Ensemble-Models/) by Dustin Rogers
| **[Publication]** [Ensemble Selection from Libraries of Models](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.2859&rep=rep1&type=pdf) by Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew and Alex Ksikes (Proceedings of the 21 st International Conference on Machine Learning,)
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
|
|
|
|